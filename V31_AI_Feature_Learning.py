import pandas as pd
import numpy as np
import time
import warnings
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import joblib

warnings.filterwarnings('ignore')

# ==============================================================================
# å…¨å±€é…ç½® (V31 AIç‰¹å¾å­¦ä¹ ç‰ˆ)
# ==============================================================================
# ä¸Šæµ·+æ·±åœ³æ•°æ®è·¯å¾„
SHANGHAI_DATA_PATH = '/Users/yamijin/Desktop/Aè‚¡ä¸»æ¿å†å²æ•°æ®_2018è‡³ä»Š_20250729_233546/ä¸Šæµ·ä¸»æ¿_å†å²æ•°æ®_2018è‡³ä»Š_20250729_233546.csv'
SHENZHEN_DATA_PATH = '/Users/yamijin/Desktop/Aè‚¡ä¸»æ¿å†å²æ•°æ®_2018è‡³ä»Š_20250729_233546/æ·±åœ³ä¸»æ¿_å†å²æ•°æ®_2018è‡³ä»Š_20250729_233546.csv'

OUTPUT_MODEL_PATH = 'V31_AI_Feature_Model.pkl'
OUTPUT_SCALER_PATH = 'V31_AI_Feature_Scaler.pkl'
OUTPUT_TRADES_PATH = 'V31_AI_Enhanced_Trades.csv'

# V6æ ¸å¿ƒå‚æ•°
V_BOTTOM_LOOKBACK = 20
STOP_LOSS_PCT = 0.031
MAX_HOLDING_DAYS = 10
FEATURE_WINDOW = 4  # ä½¿ç”¨4å¤©çª—å£æå–ç‰¹å¾

# ==============================================================================
# æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ==============================================================================
def load_combined_data():
    """åŠ è½½ä¸Šæµ·+æ·±åœ³æ•°æ®å¹¶åˆå¹¶"""
    print("å¼€å§‹åŠ è½½ä¸Šæµ·+æ·±åœ³æ•°æ®...")
    start_time = time.time()
    
    # åŠ è½½ä¸¤ä¸ªå¸‚åœºæ•°æ®
    try:
        sh_data = pd.read_csv(SHANGHAI_DATA_PATH)
        sz_data = pd.read_csv(SHENZHEN_DATA_PATH)
        print(f"ä¸Šæµ·æ•°æ®: {len(sh_data):,} æ¡")
        print(f"æ·±åœ³æ•°æ®: {len(sz_data):,} æ¡")
    except FileNotFoundError as e:
        print(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        return None
    
    # åˆå¹¶æ•°æ®
    all_data = pd.concat([sh_data, sz_data], ignore_index=True)
    
    # æ ‡å‡†åŒ–åˆ—å
    all_data.rename(columns={
        'ts_code': 'ä»£ç ', 'trade_date': 'æ—¥æœŸ', 'open': 'å¼€ç›˜',
        'high': 'æœ€é«˜', 'low': 'æœ€ä½', 'close': 'æ”¶ç›˜',
        'vol': 'æˆäº¤é‡', 'amount': 'æˆäº¤é¢'
    }, inplace=True)
    
    all_data['æ—¥æœŸ'] = pd.to_datetime(all_data['æ—¥æœŸ'], format='%Y%m%d')
    all_data.sort_values(by=['ä»£ç ', 'æ—¥æœŸ'], inplace=True)
    
    print(f"åˆå¹¶åæ€»æ•°æ®: {len(all_data):,} æ¡ï¼Œ{all_data['ä»£ç '].nunique()} åªè‚¡ç¥¨")
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
    return all_data

# ==============================================================================
# é«˜çº§ç‰¹å¾æå– (4å¤©çª—å£)
# ==============================================================================
def extract_advanced_features(stock_data, signal_idx):
    """
    æå–ä¿¡å·ç‚¹çš„é«˜çº§ç‰¹å¾ (ä½¿ç”¨4å¤©çª—å£: å½“å¤©+å‰3å¤©)
    """
    try:
        if signal_idx < FEATURE_WINDOW:
            return None
            
        # è·å–4å¤©çª—å£æ•°æ®
        window_data = stock_data.iloc[signal_idx-FEATURE_WINDOW+1:signal_idx+1]
        if len(window_data) != FEATURE_WINDOW:
            return None
            
        features = {}
        
        # 1. åŸºç¡€ä»·æ ¼ç‰¹å¾ (4å¤©)
        opens = window_data['å¼€ç›˜'].values
        highs = window_data['æœ€é«˜'].values
        lows = window_data['æœ€ä½'].values
        closes = window_data['æ”¶ç›˜'].values
        volumes = window_data['æˆäº¤é‡'].values
        amounts = window_data['æˆäº¤é¢'].values if 'æˆäº¤é¢' in window_data.columns else volumes * closes
        
        # 2. ä»·æ ¼åŠ¨é‡ç‰¹å¾
        features['price_momentum_3d'] = (closes[-1] - closes[0]) / closes[0]  # 3å¤©æ€»æ¶¨å¹…
        features['price_acceleration'] = closes[-1] - 2*closes[-2] + closes[-3]  # ä»·æ ¼åŠ é€Ÿåº¦
        features['consecutive_up_strength'] = np.sum(np.diff(closes) > 0)  # è¿ç»­ä¸Šæ¶¨å¼ºåº¦
        
        # 3. æ³¢åŠ¨ç‡ç‰¹å¾
        returns = np.diff(closes) / closes[:-1]
        features['volatility_4d'] = np.std(returns)  # 4å¤©æ³¢åŠ¨ç‡
        features['max_intraday_range'] = np.max((highs - lows) / closes)  # æœ€å¤§æ—¥å†…æ³¢åŠ¨
        features['price_stability'] = 1 / (1 + features['volatility_4d'])  # ä»·æ ¼ç¨³å®šæ€§
        
        # 4. æˆäº¤é‡ç‰¹å¾
        features['volume_trend'] = (volumes[-1] - volumes[0]) / volumes[0] if volumes[0] > 0 else 0
        features['volume_acceleration'] = volumes[-1] - 2*volumes[-2] + volumes[-3]
        features['avg_volume_4d'] = np.mean(volumes)
        features['volume_consistency'] = 1 / (1 + np.std(volumes) / np.mean(volumes)) if np.mean(volumes) > 0 else 0
        
        # 5. æˆäº¤é¢ç‰¹å¾
        features['amount_growth'] = (amounts[-1] - amounts[0]) / amounts[0] if amounts[0] > 0 else 0
        features['amount_volume_ratio'] = np.mean(amounts / volumes) if np.all(volumes > 0) else 0
        
        # 6. Kçº¿å½¢æ€ç‰¹å¾ (æœ€åä¸€å¤©)
        last_day = window_data.iloc[-1]
        body_size = abs(last_day['æ”¶ç›˜'] - last_day['å¼€ç›˜'])
        total_range = last_day['æœ€é«˜'] - last_day['æœ€ä½']
        features['body_ratio'] = body_size / total_range if total_range > 0 else 0
        features['upper_shadow'] = (last_day['æœ€é«˜'] - max(last_day['å¼€ç›˜'], last_day['æ”¶ç›˜'])) / total_range if total_range > 0 else 0
        features['lower_shadow'] = (min(last_day['å¼€ç›˜'], last_day['æ”¶ç›˜']) - last_day['æœ€ä½']) / total_range if total_range > 0 else 0
        
        # 7. ç›¸å¯¹ä½ç½®ç‰¹å¾
        features['close_vs_high_4d'] = closes[-1] / np.max(highs)  # æ”¶ç›˜ä»·ç›¸å¯¹4å¤©æœ€é«˜ä»·
        features['close_vs_low_4d'] = closes[-1] / np.min(lows)   # æ”¶ç›˜ä»·ç›¸å¯¹4å¤©æœ€ä½ä»·
        features['position_in_range'] = (closes[-1] - np.min(lows)) / (np.max(highs) - np.min(lows)) if np.max(highs) > np.min(lows) else 0.5
        
        # 8. è¶‹åŠ¿å¼ºåº¦ç‰¹å¾
        # è®¡ç®—ç®€å•ç§»åŠ¨å¹³å‡è¶‹åŠ¿
        if signal_idx >= 10:
            ma5_current = stock_data.iloc[signal_idx-4:signal_idx+1]['æ”¶ç›˜'].mean()
            ma5_prev = stock_data.iloc[signal_idx-9:signal_idx-4]['æ”¶ç›˜'].mean()
            features['ma5_trend'] = (ma5_current - ma5_prev) / ma5_prev if ma5_prev > 0 else 0
        else:
            features['ma5_trend'] = 0
            
        # 9. Vå‹åº•æ·±åº¦ç‰¹å¾ (åŸºäºV6é€»è¾‘)
        if signal_idx >= V_BOTTOM_LOOKBACK + 3:
            day0_idx = signal_idx - 3  # ç¬¬0å¤©ä½ç½®
            window_start = max(0, day0_idx - V_BOTTOM_LOOKBACK + 1)
            v_window_prices = stock_data.iloc[window_start:day0_idx+1]['æ”¶ç›˜'].values
            if len(v_window_prices) > 0:
                min_price = np.min(v_window_prices)
                max_price = np.max(v_window_prices)
                features['v_bottom_depth'] = (max_price - min_price) / max_price if max_price > 0 else 0
                features['v_recovery_ratio'] = (closes[-1] - min_price) / (max_price - min_price) if max_price > min_price else 0
            else:
                features['v_bottom_depth'] = 0
                features['v_recovery_ratio'] = 0
        else:
            features['v_bottom_depth'] = 0
            features['v_recovery_ratio'] = 0
            
        # 10. å¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾
        features['price_gaps'] = np.sum(opens[1:] != closes[:-1])  # è·³ç©ºæ¬¡æ•°
        features['closing_strength'] = np.mean((closes - lows) / (highs - lows))  # æ”¶ç›˜å¼ºåº¦
        
        return features
        
    except Exception as e:
        print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
        return None

# ==============================================================================
# V6ä¿¡å·æ£€æµ‹ä¸ç‰¹å¾æ ‡ç­¾ç”Ÿæˆ
# ==============================================================================
def generate_training_data(all_data):
    """ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼šç‰¹å¾+æ ‡ç­¾(å®é™…æ”¶ç›Šç‡)"""
    print("å¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    start_time = time.time()
    
    training_samples = []
    grouped = all_data.groupby('ä»£ç ')
    
    for code, stock_data in grouped:
        if len(stock_data) < V_BOTTOM_LOOKBACK + 10:
            continue
            
        df = stock_data.copy().reset_index(drop=True)
        closes = df['æ”¶ç›˜'].values
        
        # å¯»æ‰¾V6ä¿¡å·ç‚¹
        for i in range(V_BOTTOM_LOOKBACK + 4, len(closes) - MAX_HOLDING_DAYS):
            # V6æ¡ä»¶1: è¿ç»­3å¤©ä¸å‡
            if not (closes[i-2] <= closes[i-1] <= closes[i]):
                continue
                
            # V6æ¡ä»¶2: Vå‹åº•
            day0_idx = i - 3
            window_start = max(0, day0_idx - V_BOTTOM_LOOKBACK + 1)
            window_closes = closes[window_start:day0_idx + 1]
            if closes[day0_idx] != min(window_closes):
                continue
                
            # æå–ç‰¹å¾
            features = extract_advanced_features(df, i)
            if not features:
                continue
                
            # æ¨¡æ‹Ÿäº¤æ˜“è®¡ç®—å®é™…æ”¶ç›Š (ç®€åŒ–ç‰ˆV6é€»è¾‘)
            entry_price = closes[i]
            stop_loss_price = entry_price * (1 - STOP_LOSS_PCT)
            
            # å¯»æ‰¾æœªæ¥æ”¶ç›Š
            actual_return = 0
            for j in range(1, min(MAX_HOLDING_DAYS + 1, len(closes) - i)):
                future_close = closes[i + j]
                if future_close < stop_loss_price:
                    actual_return = (stop_loss_price - entry_price) / entry_price
                    break
                elif j == MAX_HOLDING_DAYS:
                    actual_return = (future_close - entry_price) / entry_price
                    
            # æ·»åŠ åˆ°è®­ç»ƒæ ·æœ¬
            sample = features.copy()
            sample['actual_return'] = actual_return
            sample['stock_code'] = code
            sample['signal_date'] = df.iloc[i]['æ—¥æœŸ']
            training_samples.append(sample)
    
    training_df = pd.DataFrame(training_samples)
    print(f"è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ: {len(training_df)} ä¸ªæ ·æœ¬ï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
    return training_df

# ==============================================================================
# AIæ¨¡å‹è®­ç»ƒ
# ==============================================================================
def train_ai_model(training_df):
    """è®­ç»ƒAIç‰¹å¾å­¦ä¹ æ¨¡å‹"""
    print("å¼€å§‹è®­ç»ƒAIæ¨¡å‹...")
    start_time = time.time()
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    feature_cols = [col for col in training_df.columns if col not in ['actual_return', 'stock_code', 'signal_date']]
    X = training_df[feature_cols].fillna(0)
    y = training_df['actual_return']
    
    print(f"ç‰¹å¾ç»´åº¦: {len(feature_cols)}")
    print(f"æ ·æœ¬æ•°é‡: {len(X)}")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # è®­ç»ƒæµ‹è¯•åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    
    # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
    model = RandomForestRegressor(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train, y_train)
    
    # æ¨¡å‹è¯„ä¼°
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    
    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)
    train_mse = mean_squared_error(y_train, train_pred)
    test_mse = mean_squared_error(y_test, test_pred)
    
    print(f"\n--- AIæ¨¡å‹è®­ç»ƒç»“æœ ---")
    print(f"è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
    print(f"æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
    print(f"è®­ç»ƒé›† MSE: {train_mse:.6f}")
    print(f"æµ‹è¯•é›† MSE: {test_mse:.6f}")
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\n--- Top 10 é‡è¦ç‰¹å¾ ---")
    print(feature_importance.head(10))
    
    # ä¿å­˜æ¨¡å‹
    joblib.dump(model, OUTPUT_MODEL_PATH)
    joblib.dump(scaler, OUTPUT_SCALER_PATH)
    
    print(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
    return model, scaler, feature_cols

# ==============================================================================
# AIå¢å¼ºå›æµ‹
# ==============================================================================
def run_ai_enhanced_backtest(all_data, model, scaler, feature_cols, top_percentile=0.3):
    """ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œå¢å¼ºå›æµ‹"""
    print(f"å¼€å§‹AIå¢å¼ºå›æµ‹ (Top {top_percentile*100:.0f}%)...")
    start_time = time.time()
    
    # ç”Ÿæˆæ‰€æœ‰ä¿¡å·å’ŒAIè¯„åˆ†
    all_signals = []
    grouped = all_data.groupby('ä»£ç ')
    
    for code, stock_data in grouped:
        if len(stock_data) < V_BOTTOM_LOOKBACK + 10:
            continue
            
        df = stock_data.copy().reset_index(drop=True)
        closes = df['æ”¶ç›˜'].values
        
        for i in range(V_BOTTOM_LOOKBACK + 4, len(closes) - 2):
            # V6ä¿¡å·æ£€æµ‹
            if not (closes[i-2] <= closes[i-1] <= closes[i]):
                continue
                
            day0_idx = i - 3
            window_start = max(0, day0_idx - V_BOTTOM_LOOKBACK + 1)
            window_closes = closes[window_start:day0_idx + 1]
            if closes[day0_idx] != min(window_closes):
                continue
                
            # æå–ç‰¹å¾å¹¶é¢„æµ‹
            features = extract_advanced_features(df, i)
            if not features:
                continue
                
            feature_vector = np.array([[features.get(col, 0) for col in feature_cols]])
            feature_scaled = scaler.transform(feature_vector)
            ai_score = model.predict(feature_scaled)[0]
            
            all_signals.append({
                'ä»£ç ': code,
                'æ—¥æœŸ': df.iloc[i]['æ—¥æœŸ'],
                'ä¹°å…¥ä»·æ ¼': closes[i],
                'AIè¯„åˆ†': ai_score
            })
    
    signals_df = pd.DataFrame(all_signals)
    print(f"ç”Ÿæˆ {len(signals_df)} ä¸ªAIè¯„åˆ†ä¿¡å·")
    
    if signals_df.empty:
        return pd.DataFrame()
    
    # ç­›é€‰Topä¿¡å·
    threshold = signals_df['AIè¯„åˆ†'].quantile(1 - top_percentile)
    elite_signals = signals_df[signals_df['AIè¯„åˆ†'] >= threshold].copy()
    
    print(f"ç­›é€‰å‡º {len(elite_signals)} ä¸ªç²¾è‹±ä¿¡å· (é˜ˆå€¼: {threshold:.4f})")
    
    # æ‰§è¡Œå›æµ‹ (ç®€åŒ–ç‰ˆ)
    trades = []
    active_trades = {}
    
    unique_dates = sorted(elite_signals['æ—¥æœŸ'].unique())
    stock_groups = all_data.groupby('ä»£ç ')
    
    for current_date in unique_dates:
        # æ¸…ç†åˆ°æœŸæŒä»“
        ended_stocks = [code for code, end_date in active_trades.items() if current_date >= end_date]
        for code in ended_stocks:
            del active_trades[code]
            
        daily_signals = elite_signals[elite_signals['æ—¥æœŸ'] == current_date]
        
        for _, signal in daily_signals.iterrows():
            stock_code = signal['ä»£ç ']
            if stock_code in active_trades:
                continue
                
            if stock_code not in stock_groups.groups:
                continue
                
            stock_data = stock_groups.get_group(stock_code)
            future_data = stock_data[stock_data['æ—¥æœŸ'] > current_date].head(MAX_HOLDING_DAYS + 2)
            
            if len(future_data) < 1:
                continue
                
            # ç®€åŒ–äº¤æ˜“æ¨¡æ‹Ÿ
            entry_price = signal['ä¹°å…¥ä»·æ ¼']
            stop_loss_price = entry_price * (1 - STOP_LOSS_PCT)
            
            exit_price = None
            exit_date = None
            exit_reason = "æŒæœ‰åˆ°æœŸ"
            
            for i in range(min(len(future_data), MAX_HOLDING_DAYS)):
                current_close = future_data.iloc[i]['æ”¶ç›˜']
                if current_close < stop_loss_price:
                    exit_price = stop_loss_price
                    exit_date = future_data.iloc[i]['æ—¥æœŸ']
                    exit_reason = "æ­¢æŸ"
                    break
                    
            if exit_price is None:
                exit_idx = min(MAX_HOLDING_DAYS - 1, len(future_data) - 1)
                exit_price = future_data.iloc[exit_idx]['æ”¶ç›˜']
                exit_date = future_data.iloc[exit_idx]['æ—¥æœŸ']
            
            total_return = (exit_price - entry_price) / entry_price
            
            trades.append({
                'ä»£ç ': stock_code,
                'ä¹°å…¥æ—¥æœŸ': current_date,
                'ä¹°å…¥ä»·æ ¼': entry_price,
                'å–å‡ºæ—¥æœŸ': exit_date,
                'å–å‡ºä»·æ ¼': exit_price,
                'æ”¶ç›Šç‡': total_return,
                'é€€å‡ºåŸå› ': exit_reason,
                'AIè¯„åˆ†': signal['AIè¯„åˆ†']
            })
            
            active_trades[stock_code] = exit_date
    
    trades_df = pd.DataFrame(trades)
    print(f"AIå¢å¼ºå›æµ‹å®Œæˆ: {len(trades_df)} ç¬”äº¤æ˜“ï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
    return trades_df

# ==============================================================================
# æ€§èƒ½åˆ†æ
# ==============================================================================
def analyze_ai_performance(trades_df):
    """åˆ†æAIå¢å¼ºç­–ç•¥æ€§èƒ½"""
    if trades_df.empty:
        print("AIå›æµ‹æœªäº§ç”Ÿä»»ä½•äº¤æ˜“")
        return
        
    total_trades = len(trades_df)
    win_trades = trades_df[trades_df['æ”¶ç›Šç‡'] > 0]
    win_rate = len(win_trades) / total_trades
    avg_return = trades_df['æ”¶ç›Šç‡'].mean()
    avg_win = win_trades['æ”¶ç›Šç‡'].mean() if not win_trades.empty else 0
    avg_loss = trades_df[trades_df['æ”¶ç›Šç‡'] <= 0]['æ”¶ç›Šç‡'].mean()
    
    print("\n" + "="*70)
    print("V31 AIç‰¹å¾å­¦ä¹ ç­–ç•¥æ€§èƒ½åˆ†æ")
    print("="*70)
    print(f"æ€»äº¤æ˜“æ¬¡æ•°: {total_trades:,}")
    print(f"èƒœç‡: {win_rate:.2%}")
    print(f"æœŸæœ›æ”¶ç›Š: {avg_return:.4%}")
    print(f"å¹³å‡ç›ˆåˆ©: {avg_win:.4%}")
    print(f"å¹³å‡äºæŸ: {avg_loss:.4%}")
    if avg_loss != 0:
        print(f"ç›ˆäºæ¯”: {-avg_win / avg_loss:.2f}")
    
    print(f"\n--- ä¸åŸºå‡†å¯¹æ¯” ---")
    print(f"V6åŸºå‡†(3å¤©ç‰ˆ): 1.89%")
    print(f"V31 AIå¢å¼º: {avg_return:.4%}")
    
    if avg_return >= 0.02:
        print("ğŸ‰ çªç ´2%ç›®æ ‡ï¼")
    elif avg_return > 0.0189:
        print("âœ… è¶…è¶ŠV6åŸºå‡†ï¼")
    else:
        print("âŒ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

# ==============================================================================
# ä¸»å‡½æ•°
# ==============================================================================
def main():
    # 1. åŠ è½½æ•°æ®
    all_data = load_combined_data()
    if all_data is None:
        return
        
    # 2. ç”Ÿæˆè®­ç»ƒæ•°æ®
    training_df = generate_training_data(all_data)
    if training_df.empty:
        print("è®­ç»ƒæ•°æ®ç”Ÿæˆå¤±è´¥")
        return
        
    # 3. è®­ç»ƒAIæ¨¡å‹
    model, scaler, feature_cols = train_ai_model(training_df)
    
    # 4. AIå¢å¼ºå›æµ‹
    trades_df = run_ai_enhanced_backtest(all_data, model, scaler, feature_cols)
    
    # 5. ä¿å­˜å’Œåˆ†æç»“æœ
    if not trades_df.empty:
        trades_df.to_csv(OUTPUT_TRADES_PATH, index=False, encoding='utf-8-sig')
        analyze_ai_performance(trades_df)
    
    print(f"\næ¨¡å‹å·²ä¿å­˜: {OUTPUT_MODEL_PATH}")
    print(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {OUTPUT_SCALER_PATH}")

if __name__ == '__main__':
    main()